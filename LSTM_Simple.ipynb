{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2MZLbSOeiyx",
        "outputId": "b1916ab4-a5d8-44ad-83e2-514049c4daf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0005199749139137566\n",
            "Epoch 2, Loss: 0.0002455409849062562\n",
            "Epoch 3, Loss: 0.00015889335190877318\n",
            "Epoch 4, Loss: 0.00011264643399044871\n",
            "Epoch 5, Loss: 8.40390202938579e-05\n",
            "Epoch 6, Loss: 6.48477507638745e-05\n",
            "Epoch 7, Loss: 5.1377883210079744e-05\n",
            "Epoch 8, Loss: 4.172238186583854e-05\n",
            "Epoch 9, Loss: 3.4450891689630225e-05\n",
            "Epoch 10, Loss: 2.884823152271565e-05\n",
            "The predicted next characters are: 0100\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Prepare the text and create character to integer mapping\n",
        "raw_text = \"000100010001000100010001000100010001000100010001000100010001000100010001\"\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "# Convert characters to integers\n",
        "int_text = [char_to_int[char] for char in raw_text]\n",
        "\n",
        "# Prepare input and target sequences for training\n",
        "input_seq = torch.tensor(int_text, dtype=torch.float32).reshape(-1, 1, 1)\n",
        "target_seq = torch.tensor(int_text, dtype=torch.long).reshape(-1, 1, 1)\n",
        "\n",
        "# Create a list of tuples (input, target) for training\n",
        "train_data = [(input_seq[i], target_seq[i]) for i in range(len(input_seq))]\n",
        "\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.out = ''\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "    def test_model(self, input_str, char_to_int, int_to_char):\n",
        "      model.eval()  # Set the model to evaluation mode\n",
        "      input_str = input_str.lower()  # Convert string to lowercase\n",
        "      int_text = [char_to_int[char] for char in input_str]  # Convert input string to integer sequence\n",
        "\n",
        "      output_str = \"\"  # Initialize the output string\n",
        "\n",
        "      with torch.no_grad():  # Deactivate gradients for the following code\n",
        "          for i in int_text:\n",
        "              # Prepare the input tensor and reshape it\n",
        "              input_tensor = torch.tensor(i, dtype=torch.float32).reshape(1, 1, 1)\n",
        "              # Forward pass\n",
        "              output = model(input_tensor)\n",
        "              # Get the index of the max value among the predicted character probabilities\n",
        "              _, predicted_idx = torch.max(output, 1)\n",
        "              # Convert the predicted index to its corresponding character and add it to the output string\n",
        "              output_str += int_to_char[predicted_idx.item()]\n",
        "\n",
        "      return output_str\n",
        "\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "# Hidden layer Hyperparameter defined here\n",
        "input_dim = 1\n",
        "hidden_dim = 160\n",
        "output_dim = len(chars)  # Number of unique characters\n",
        "\n",
        "model = SimpleLSTM(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
        "\n",
        "# Train the model\n",
        "# Set number of epochs here\n",
        "for epoch in range(10):\n",
        "    for seq, label in train_data:\n",
        "        seq = seq.view(1, 1, 1)\n",
        "        label = label.view(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(seq)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "# Create an integer to character mapping\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# Test the model with a new string\n",
        "result = model.test_model(\"0100\", char_to_int, int_to_char)\n",
        "print(f\"The predicted next characters are: {result}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}